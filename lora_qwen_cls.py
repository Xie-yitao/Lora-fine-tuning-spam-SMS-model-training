import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from peft import (
    get_peft_model,
    LoraConfig,
    TaskType
)
import numpy as np
from sklearn.metrics import accuracy_score
import model_config_qwen  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
import os

# ç¯å¢ƒé…ç½®
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
os.environ["TRUST_REMOTE_CODE"] = "true"
model_name = "qwen/Qwen2-0.5B-Instruct"
# ä»£ç†è®¾ç½®ï¼ˆæ ¹æ®éœ€è¦é…ç½®ï¼‰
proxies = {
    "http": "http://127.0.0.1:7890",
    "https": "http://127.0.0.1:7890",
}

# ========== é…ç½®ç±» ==========
class ModelConfig:
    model_path = model_config_qwen.model_name_tokenizer_path   # æ¨¡å‹ä¿å­˜è·¯å¾„
    num_labels = model_config_qwen.num_labels                 # åˆ†ç±»ç±»åˆ«æ•°
    model_min_length = model_config_qwen.model_min_length     # æœ€å°è¾“å…¥é•¿åº¦
    freeze_base_model = model_config_qwen.freeze_base_model   # æ˜¯å¦å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°ï¼ˆåªè®­ç»ƒ LoRA å‚æ•°ï¼‰
    pooling_type = model_config_qwen.pooling_type             # æ± åŒ–æ–¹å¼ï¼ˆå¦‚ last, first, meanï¼‰

class TrainingConfig:
    output_dir = model_config_qwen.output_dir                 # è¾“å‡ºç›®å½•
    max_epochs = model_config_qwen.max_epochs                 # æœ€å¤§è®­ç»ƒè½®æ¬¡
    batch_size = model_config_qwen.train_batch_size           # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    eval_batch_size = model_config_qwen.test_batch_size         # éªŒè¯æ‰¹æ¬¡å¤§å°
    learning_rate = model_config_qwen.learning_rate           # å­¦ä¹ ç‡
    weight_decay = model_config_qwen.weight_decay             # æƒé‡è¡°å‡
    gradient_accumulation = model_config_qwen.gradient_accumulation_steps  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    fp16 = model_config_qwen.use_fp16                         # æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦
    gradient_checkpointing = model_config_qwen.gradient_checkpointing  # æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    early_stop_patience = model_config_qwen.early_stop_patience           # æ—©åœè€å¿ƒå€¼

class LoRAConfig:
    r = model_config_qwen.lora_r                  # LoRA ç§©
    lora_alpha = model_config_qwen.lora_alpha     # LoRA alpha å€¼
    lora_dropout = model_config_qwen.lora_dropout # LoRA dropout ç‡
    target_modules = model_config_qwen.target_modules  # LoRA æ‰€é’ˆå¯¹çš„ç›®æ ‡æ¨¡å—åˆ—è¡¨

# ========== å·¥å…·å‡½æ•° ==========
def analyze_text_length(texts, tokenizer):
    """åˆ†ææ–‡æœ¬é•¿åº¦åˆ†å¸ƒï¼Œå¹¶åŠ¨æ€ç¡®å®š max_length"""
    lengths = []
    for text in texts:
        tokens = tokenizer.tokenize(text)
        lengths.append(len(tokens))
    print(f"\nğŸ“Š æ–‡æœ¬é•¿åº¦åˆ†æ:")
    print(f"æœ€å°: {min(lengths)}, æœ€å¤§: {max(lengths)}, å¹³å‡: {np.mean(lengths):.1f}")
    print(f"75% åˆ†ä½æ•°: {np.percentile(lengths, 75)}, 95% åˆ†ä½æ•°: {np.percentile(lengths, 95)}")
    max_length = min(int(np.percentile(lengths, 95)), tokenizer.model_max_length)
    max_length = max(max_length, ModelConfig.model_min_length)
    print(f"æ¨è max_length: {max_length}")
    return max_length

def prepare_dataset(tokenizer, texts, labels, max_length):
    """æ•°æ®é›†é¢„å¤„ç†ï¼Œå°†æ–‡æœ¬ token åŒ–ï¼ŒåŒæ—¶ä¿ç•™æ ‡ç­¾"""
    dataset = Dataset.from_dict({"text": texts, "label": labels})
    
    def tokenize_fn(examples):
        tokenized = tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )
        return {
            "input_ids": tokenized["input_ids"][0],
            "attention_mask": tokenized["attention_mask"][0],
            "label": examples["label"]
        }
    
    return dataset.map(
        tokenize_fn,
        batched=False,
        remove_columns=["text"],
        num_proc=4,
        desc="Tokenizing"
    )

# ========== ä¸»æµç¨‹ ==========
def main():
    # ç¡¬ä»¶æ£€æŸ¥
    print(f"\nğŸ–¥ï¸ ç¡¬ä»¶é…ç½®æ£€æŸ¥:")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

    # åˆå§‹åŒ–é…ç½®
    model_config = ModelConfig()
    train_config = TrainingConfig()
    lora_config = LoRAConfig()

    # åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_df = pd.read_excel(model_config_qwen.train_data_path)
    valid_df = pd.read_excel(model_config_qwen.dev_data_path)
    print(f"è®­ç»ƒé›†: {len(train_df)} æ¡, éªŒè¯é›†: {len(valid_df)} æ¡")

    # åˆå§‹åŒ– Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        proxies=proxies,
        pad_token="<|endoftext|>"
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åŠ¨æ€ç¡®å®š max_length
    max_length = analyze_text_length(train_df.iloc[:, 0].tolist(), tokenizer)

    # å‡†å¤‡æ•°æ®é›†
    train_dataset = prepare_dataset(
        tokenizer,
        train_df.iloc[:, 0].tolist(),
        train_df.iloc[:, 1].tolist(),
        max_length
    )
    valid_dataset = prepare_dataset(
        tokenizer,
        valid_df.iloc[:, 0].tolist(),
        valid_df.iloc[:, 1].tolist(),
        max_length
    )

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=model_config.num_labels,
        id2label={"0": "éåƒåœ¾çŸ­ä¿¡", "1": "åƒåœ¾çŸ­ä¿¡"},
        label2id={"éåƒåœ¾çŸ­ä¿¡": 0, "åƒåœ¾çŸ­ä¿¡": 1},
        # torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        proxies=proxies,
        use_cache=False  # ç¦ç”¨ç¼“å­˜ä»¥å…¼å®¹æ¢¯åº¦æ£€æŸ¥ç‚¹
    )
    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    # åº”ç”¨ LoRA å¾®è°ƒé…ç½®
    print("\nğŸ›ï¸ åº”ç”¨ LoRA é…ç½®...")
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_CLS,
        target_modules=lora_config.target_modules,
        inference_mode=False,
        r=lora_config.r,
        lora_alpha=lora_config.lora_alpha,
        lora_dropout=lora_config.lora_dropout,
        modules_to_save=["score"]  # ä¿ç•™åˆ†ç±»å¤´å‚æ•°å¯è®­ç»ƒ
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
    print("\nğŸ” å¯è®­ç»ƒå‚æ•°æ£€æŸ¥:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"å¯è®­ç»ƒå‚æ•°: {name}")

    # é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=train_config.output_dir,
        eval_strategy="epoch",  # ä¿®æ­£å‚æ•°åç§°ä¸ºevaluation_strategyï¼ŒæŠ¥é”™å°±æ”¹ä¸ºeval_strategy
        save_strategy="epoch",
        learning_rate=train_config.learning_rate,
        per_device_train_batch_size=train_config.batch_size,
        per_device_eval_batch_size=train_config.eval_batch_size,
        gradient_accumulation_steps=train_config.gradient_accumulation,
        num_train_epochs=train_config.max_epochs,
        weight_decay=train_config.weight_decay,
        fp16=train_config.fp16,
        gradient_checkpointing=train_config.gradient_checkpointing,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        logging_steps=50,
        save_total_limit=2,
        report_to="none",
        optim="adamw_torch",
        dataloader_num_workers=4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1
    )

    # è¯„ä¼°å‡½æ•°ï¼ˆè®¡ç®—å‡†ç¡®ç‡ï¼‰
    def compute_metrics(p):
        preds = np.argmax(p.predictions, axis=1)
        return {"accuracy": accuracy_score(p.label_ids, preds)}

    # æ„å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=train_config.early_stop_patience)]
    )

    # å¼€å§‹å¾®è°ƒè®­ç»ƒ
    print("\nğŸš‚ å¯åŠ¨å¾®è°ƒ...")
    train_result = trainer.train()

    # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œ LoRA æƒé‡
    print("\nğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹...")
    best_model_dir = os.path.join(train_config.output_dir, "best_model")
    model.save_pretrained(best_model_dir)
    tokenizer.save_pretrained(best_model_dir)
    lora_model_dir = model_config_qwen.model_path_lora
    model.save_pretrained(lora_model_dir)
    print(f"LoRA æƒé‡ä¿å­˜è‡³: {lora_model_dir}")

if __name__ == "__main__":
    torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨ TF32 çŸ©é˜µè¿ç®—
    torch.backends.cudnn.benchmark = True         # å¯ç”¨ cuDNN åŸºå‡†ä¼˜åŒ–
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # å‡å°‘æ˜¾å­˜ç¢ç‰‡
    main()


