# import torch
# import pandas as pd
# from datasets import Dataset
# from transformers import (
#     AutoTokenizer,
#     AutoModelForCausalLM,
#     AutoConfig,
#     TrainingArguments,
#     Trainer,
#     EarlyStoppingCallback
# )
# from peft import (
#     get_peft_model,
#     LoraConfig,
#     TaskType
# )
# import transformers
# import numpy as np
# from sklearn.metrics import accuracy_score
# from torch.nn import Linear, CrossEntropyLoss
# from transformers.utils import ModelOutput
# from typing import Optional, Tuple
# from dataclasses import dataclass
# import model_config_qwen  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
# import os

# # ç¯å¢ƒé…ç½®
# os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
# os.environ["TRUST_REMOTE_CODE"] = "true"

# # ========== é…ç½®ç±» ==========
# class ModelConfig:
#     model_path = model_config_qwen.model_name_tokenizer_path               # æ¨¡å‹è·¯å¾„
#     num_labels = model_config_qwen.num_labels                              # åˆ†ç±»ç±»åˆ«æ•°
#     model_min_length = model_config_qwen.model_min_length                  # æœ€å°è¾“å…¥é•¿åº¦
#     freeze_base_model = model_config_qwen.freeze_base_model                # å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°
#     pooling_type = model_config_qwen.pooling_type                          # [last, first, mean] æ± åŒ–æ–¹å¼

# class TrainingConfig:
#     output_dir = model_config_qwen.output_dir                              # è¾“å‡ºç›®å½•
#     max_epochs = model_config_qwen.max_epochs                              # æœ€å¤§è®­ç»ƒè½®æ¬¡
#     batch_size = model_config_qwen.train_batch_size                        # è®­ç»ƒæ‰¹æ¬¡å¤§å°(V100 32Gå¯è°ƒæ•´åˆ°24-32)
#     eval_batch_size = model_config_qwen.test_batch_size                    # éªŒè¯æ‰¹æ¬¡å¤§å°
#     learning_rate = model_config_qwen.learning_rate                        # å­¦ä¹ ç‡
#     weight_decay = model_config_qwen.weight_decay                          # æƒé‡è¡°å‡
#     gradient_accumulation = model_config_qwen.gradient_accumulation_steps  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
#     fp16 = model_config_qwen.use_fp16                                      # å¯ç”¨æ··åˆç²¾åº¦
#     gradient_checkpointing = model_config_qwen.gradient_checkpointing      # æ¢¯åº¦æ£€æŸ¥ç‚¹
#     early_stop_patience = model_config_qwen.early_stop_patience            # æ—©åœè€å¿ƒå€¼

# class LoRAConfig:
#     r = model_config_qwen.lora_r                                           # LoRAç§©
#     lora_alpha = model_config_qwen.lora_alpha                              # LoRA alphaå€¼
#     lora_dropout = model_config_qwen.lora_dropout                          # Dropoutç‡
#     target_modules = model_config_qwen.target_modules                      # ç›®æ ‡æ¨¡å—

# # ========== æ•°æ®ç±»å®šä¹‰ ==========
# @dataclass
# class SequenceClassifierOutput(ModelOutput):
#     loss: Optional[torch.FloatTensor] = None
#     logits: torch.FloatTensor = None
#     hidden_states: Optional[Tuple[torch.FloatTensor]] = None
#     attentions: Optional[Tuple[torch.FloatTensor]] = None

# # ========== æ¨¡å‹ç±» ==========
# class QwenForSequenceClassification(transformers.PreTrainedModel):
#     supports_gradient_checkpointing = True  # æ–°å¢
#     def __init__(self, config):
#         super().__init__(config)
#         self.num_labels = config.num_labels
#         self.model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
        
#         # åˆ†ç±»å¤´
#         self.classifier = Linear(config.hidden_size, config.num_labels)
#         self.pooling_type = ModelConfig.pooling_type
        
#         # å†»ç»“åŸºç¡€æ¨¡å‹
#         if ModelConfig.freeze_base_model:
#             for param in self.model.parameters():
#                 param.requires_grad = False
        
#         self.post_init()

#     def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
#         outputs = self.model(
#             input_ids=input_ids,
#             attention_mask=attention_mask,
#             output_hidden_states=True,
#             use_cache=False  # ç¦ç”¨ç¼“å­˜èŠ‚çœæ˜¾å­˜
#         )
        
#         # ç‰¹å¾æ± åŒ–
#         last_hidden = outputs.hidden_states[-1]
#         if self.pooling_type == "first":
#             pooled = last_hidden[:, 0, :]
#         elif self.pooling_type == "mean":
#             pooled = last_hidden.mean(dim=1)
#         else:  # last
#             pooled = last_hidden[torch.arange(last_hidden.size(0)), (attention_mask != 0).sum(dim=1)-1]
        
#         logits = self.classifier(pooled)
#         loss = None
#         if labels is not None:
#             loss_fct = CrossEntropyLoss()
#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

#         return SequenceClassifierOutput(
#             loss=loss,
#             logits=logits,
#             hidden_states=outputs.hidden_states,
#             attentions=outputs.attentions
#         )

# # ========== å·¥å…·å‡½æ•° ==========
# def analyze_text_length(texts, tokenizer):
#     """åˆ†ææ–‡æœ¬é•¿åº¦åˆ†å¸ƒ"""
#     lengths = []
#     for text in texts:
#         tokens = tokenizer.tokenize(text)
#         lengths.append(len(tokens))
    
#     print(f"\nğŸ“Š æ–‡æœ¬é•¿åº¦åˆ†æ:")
#     print(f"æœ€å°: {min(lengths)}, æœ€å¤§: {max(lengths)}, å¹³å‡: {np.mean(lengths):.1f}")
#     print(f"75%åˆ†ä½æ•°: {np.percentile(lengths, 75)}, 95%åˆ†ä½æ•°: {np.percentile(lengths, 95)}")
    
#     # åŠ¨æ€ç¡®å®šmax_length
#     max_length = min(
#         int(np.percentile(lengths, 95)), 
#         tokenizer.model_max_length
#     )
#     max_length = max(max_length, ModelConfig.model_min_length)
#     print(f"æ¨èmax_length: {max_length}")
#     return max_length

# def prepare_dataset(tokenizer, texts, labels, max_length):
#     """æ•°æ®é›†é¢„å¤„ç†"""
#     dataset = Dataset.from_dict({"text": texts, "label": labels})
    
#     def tokenize_fn(examples):
#         tokenized = tokenizer(
#             examples["text"],
#             padding="max_length",
#             truncation=True,
#             max_length=max_length,
#             return_tensors="pt",
#         )
#         return {
#             "input_ids": tokenized["input_ids"][0],
#             "attention_mask": tokenized["attention_mask"][0],
#             "label": examples["label"]
#         }
    
#     return dataset.map(
#         tokenize_fn,
#         batched=False,
#         remove_columns=["text"],
#         num_proc=4,
#         desc="Tokenizing"
#     )

# # ========== ä¸»æµç¨‹ ==========
# def main():
#     # ç¡¬ä»¶æ£€æŸ¥
#     print(f"\nğŸ–¥ï¸ ç¡¬ä»¶é…ç½®æ£€æŸ¥:")
#     print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
#     print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
#     if torch.cuda.is_available():
#         print(f"GPU: {torch.cuda.get_device_name(0)}")
#         print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")

#     # åˆå§‹åŒ–é…ç½®
#     model_config = ModelConfig()
#     train_config = TrainingConfig()
#     lora_config = LoRAConfig()

#     # åŠ è½½æ•°æ®
#     print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
#     train_df = pd.read_excel(model_config_qwen.train_data_path)
#     valid_df = pd.read_excel(model_config_qwen.dev_data_path)
#     print(f"è®­ç»ƒé›†: {len(train_df)}æ¡, éªŒè¯é›†: {len(valid_df)}æ¡")

#     # åˆå§‹åŒ–Tokenizer
#     tokenizer = AutoTokenizer.from_pretrained(
#         model_config.model_path,
#         trust_remote_code=True,
#         pad_token="<|endoftext|>"
#     )

#     # åŠ¨æ€ç¡®å®šmax_length
#     max_length = analyze_text_length(
#         train_df.iloc[:, 0].tolist(),
#         tokenizer
#     )

#     # å‡†å¤‡æ•°æ®é›†
#     train_dataset = prepare_dataset(
#         tokenizer,
#         train_df.iloc[:, 0].tolist(),
#         train_df.iloc[:, 1].tolist(),
#         max_length
#     )
#     valid_dataset = prepare_dataset(
#         tokenizer,
#         valid_df.iloc[:, 0].tolist(),
#         valid_df.iloc[:, 1].tolist(),
#         max_length
#     )

#     # åŠ è½½åŸºç¡€æ¨¡å‹
#     print("\nğŸš€ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
#     config = AutoConfig.from_pretrained(
#         model_config.model_path,
#         num_labels=model_config.num_labels,
#         trust_remote_code=True
#     )
#     model = QwenForSequenceClassification(config)
    
#     # åº”ç”¨LoRA
#     print("\nğŸ›ï¸ åº”ç”¨LoRAé…ç½®...")
#     peft_config = LoraConfig(
#         r=lora_config.r,
#         lora_alpha=lora_config.lora_alpha,
#         lora_dropout=lora_config.lora_dropout,
#         target_modules=lora_config.target_modules,
#         task_type=TaskType.SEQ_CLS,
#         inference_mode=False,
#         bias="none"
#     )
#     model = get_peft_model(model, peft_config)
#     model.print_trainable_parameters()

#     # è®­ç»ƒå‚æ•°é…ç½®
#     training_args = TrainingArguments(
#         output_dir=train_config.output_dir,
#         evaluation_strategy="epoch",
#         save_strategy="epoch",
#         learning_rate=train_config.learning_rate,
#         per_device_train_batch_size=train_config.batch_size,
#         per_device_eval_batch_size=train_config.eval_batch_size,
#         gradient_accumulation_steps=train_config.gradient_accumulation,
#         num_train_epochs=train_config.max_epochs,
#         weight_decay=train_config.weight_decay,
#         fp16=train_config.fp16,
#         gradient_checkpointing=train_config.gradient_checkpointing,
#         load_best_model_at_end=True,
#         metric_for_best_model="accuracy",
#         logging_steps=50,
#         save_total_limit=2,
#         report_to="none",
#         optim="adamw_torch",
#         dataloader_num_workers=4,
#         lr_scheduler_type="cosine",
#         warmup_ratio=0.1
#     )

#     # è¯„ä¼°å‡½æ•°
#     def compute_metrics(p):
#         preds = np.argmax(p.predictions, axis=1)
#         return {"accuracy": accuracy_score(p.label_ids, preds)}

#     # è®­ç»ƒå™¨
#     trainer = Trainer(
#         model=model,
#         args=training_args,
#         train_dataset=train_dataset,
#         eval_dataset=valid_dataset,
#         compute_metrics=compute_metrics,
#         callbacks=[
#             EarlyStoppingCallback(early_stopping_patience=train_config.early_stop_patience)
#         ]
#     )

#     # å¼€å§‹è®­ç»ƒ
#     print("\nğŸš‚ å¯åŠ¨è®­ç»ƒ...")
#     train_result = trainer.train()
    
#     # ä¿å­˜æ¨¡å‹
#     print("\nğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹...")
#     model.save_pretrained(os.path.join(train_config.output_dir, "best_model"))
#     tokenizer.save_pretrained(os.path.join(train_config.output_dir, "best_model"))
#     # æ˜¾å¼ä¿å­˜LoRAæƒé‡åˆ°æŒ‡å®šè·¯å¾„ 
#     model.save_pretrained(model_config_qwen.model_path_lora)
#     print(f"LoRA weights saved to: {model_config_qwen.model_path_lora}")

# if __name__ == "__main__":
#     # æ˜¾å­˜ä¼˜åŒ–é…ç½®
#     torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨TF32çŸ©é˜µè¿ç®—
#     torch.backends.cudnn.benchmark = True        # å¯ç”¨cuDNNåŸºå‡†ä¼˜åŒ–
#     os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # å‡å°‘æ˜¾å­˜ç¢ç‰‡
    
#     main()



import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoConfig,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from peft import (
    get_peft_model,
    LoraConfig,
    TaskType
)
import transformers
import numpy as np
from sklearn.metrics import accuracy_score
from torch.nn import Linear, CrossEntropyLoss
from transformers.utils import ModelOutput
from typing import Optional, Tuple
from dataclasses import dataclass
import model_config_qwen  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
import os

# ç¯å¢ƒé…ç½®
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
os.environ["TRUST_REMOTE_CODE"] = "true"

# ========== é…ç½®ç±» ==========
class ModelConfig:
    model_path = model_config_qwen.model_name_tokenizer_path               
    num_labels = model_config_qwen.num_labels                              
    model_min_length = model_config_qwen.model_min_length                  
    freeze_base_model = model_config_qwen.freeze_base_model                
    pooling_type = model_config_qwen.pooling_type                          # [last, first, mean]

class TrainingConfig:
    output_dir = model_config_qwen.output_dir                              
    max_epochs = model_config_qwen.max_epochs                              
    batch_size = model_config_qwen.train_batch_size                        
    eval_batch_size = model_config_qwen.test_batch_size                    
    learning_rate = model_config_qwen.learning_rate                        
    weight_decay = model_config_qwen.weight_decay                          
    gradient_accumulation = model_config_qwen.gradient_accumulation_steps  
    fp16 = model_config_qwen.use_fp16                                      
    gradient_checkpointing = model_config_qwen.gradient_checkpointing      
    early_stop_patience = model_config_qwen.early_stop_patience            

class LoRAConfig:
    r = model_config_qwen.lora_r                                           
    lora_alpha = model_config_qwen.lora_alpha                              
    lora_dropout = model_config_qwen.lora_dropout                          
    target_modules = model_config_qwen.target_modules                      

# ========== æ•°æ®ç±»å®šä¹‰ ==========
@dataclass
class SequenceClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None

# ========== æ¨¡å‹ç±» ==========
class QwenForSequenceClassification(transformers.PreTrainedModel):
    supports_gradient_checkpointing = True
    
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        
        # å…³é”®ä¿®å¤ï¼šæ˜¾å¼è®¾ç½®æ¨¡å‹ç±»å‹
        config.model_type = "qwen"
        config.architectures = ["QwenForSequenceClassification"]
        
        # åˆå§‹åŒ–åŸºç¡€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
        self.classifier = Linear(config.hidden_size, config.num_labels)
        self.pooling_type = ModelConfig.pooling_type
        
        # å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°
        if ModelConfig.freeze_base_model:
            for param in self.model.parameters():
                param.requires_grad = False
                
        self.post_init()

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, AutoModelForCausalLM):
            module.gradient_checkpointing = value

    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        # æ˜¾å¼ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœæ˜¾å­˜
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            use_cache=False  # å…³é”®å‚æ•°
        )
        
        # ä¼˜åŒ–ç‰¹å¾æ± åŒ–
        last_hidden = outputs.hidden_states[-1]
        if self.pooling_type == "first":
            pooled = last_hidden[:, 0, :]
        elif self.pooling_type == "mean":
            pooled = last_hidden.mean(dim=1)
        else:
            pooled = last_hidden[torch.arange(last_hidden.size(0)), (attention_mask != 0).sum(dim=1)-1]
        
        # åˆ†ç±»å¤´
        logits = self.classifier(pooled)
        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions
        )

# ========== å·¥å…·å‡½æ•° ==========
def analyze_text_length(texts, tokenizer):
    """åŠ¨æ€ç¡®å®šmax_length"""
    lengths = [len(tokenizer.tokenize(text)) for text in texts]
    print(f"\nğŸ“Š æ–‡æœ¬é•¿åº¦åˆ†æï¼ˆ95%åˆ†ä½æ•°: {np.percentile(lengths, 95):.0f}ï¼‰")
    return max(
        min(int(np.percentile(lengths, 95)), 
        tokenizer.model_max_length
    ), ModelConfig.model_min_length)

def prepare_dataset(tokenizer, texts, labels, max_length):
    """å¸¦æ˜¾å­˜ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†"""
    dataset = Dataset.from_dict({"text": texts, "label": labels})
    
    def tokenize_fn(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )
    
    return dataset.map(
        tokenize_fn,
        batched=True,
        remove_columns=["text"],
        num_proc=4,
        desc="Tokenizing",
        batch_size=100  # æ‰¹å¤„ç†æå‡å¤„ç†é€Ÿåº¦
    )

# ========== ä¸»æµç¨‹ ==========
def main():
    # ç¡¬ä»¶æ£€æŸ¥
    print(f"\nğŸ–¥ï¸ ç¡¬ä»¶æ£€æŸ¥:")
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"å¯ç”¨æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")

    # åˆå§‹åŒ–é…ç½®
    model_config = ModelConfig()
    train_config = TrainingConfig()
    lora_config = LoRAConfig()

    # æ•°æ®åŠ è½½
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_df = pd.read_excel(model_config_qwen.train_data_path)
    valid_df = pd.read_excel(model_config_qwen.dev_data_path)
    print(f"è®­ç»ƒé›†: {len(train_df)}æ¡ | éªŒè¯é›†: {len(valid_df)}æ¡")

    # åˆå§‹åŒ–Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_config.model_path,
        trust_remote_code=True,
        pad_token="<|endoftext|>"
    )

    # åŠ¨æ€ç¡®å®šmax_length
    max_length = analyze_text_length(train_df.iloc[:, 0].tolist(), tokenizer)
    print(f"ä½¿ç”¨max_length: {max_length}")

    # å‡†å¤‡æ•°æ®é›†
    train_dataset = prepare_dataset(tokenizer, train_df.iloc[:, 0].tolist(), train_df.iloc[:, 1].tolist(), max_length)
    valid_dataset = prepare_dataset(tokenizer, valid_df.iloc[:, 0].tolist(), valid_df.iloc[:, 1].tolist(), max_length)

    # æ¨¡å‹é…ç½®
    config = AutoConfig.from_pretrained(
        model_config.model_path,
        num_labels=model_config.num_labels,
        trust_remote_code=True
    )
    # å…³é”®ä¿®å¤ï¼šç¡®ä¿ä¿å­˜çš„é…ç½®åŒ…å«æ¨¡å‹ç±»å‹
    config.model_type = "qwen"
    config.architectures = ["QwenForSequenceClassification"]

    # åˆå§‹åŒ–æ¨¡å‹
    print("\nğŸš€ åˆå§‹åŒ–æ¨¡å‹...")
    model = QwenForSequenceClassification(config)
    
    # åº”ç”¨LoRA
    print("\nğŸ›ï¸ åº”ç”¨LoRAé…ç½®...")
    peft_config = LoraConfig(
        r=lora_config.r,
        lora_alpha=lora_config.lora_alpha,
        lora_dropout=lora_config.lora_dropout,
        target_modules=lora_config.target_modules,
        task_type=TaskType.SEQ_CLS,
        inference_mode=False,
        bias="none"
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=train_config.output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=train_config.learning_rate,
        per_device_train_batch_size=train_config.batch_size,
        per_device_eval_batch_size=train_config.eval_batch_size,
        gradient_accumulation_steps=train_config.gradient_accumulation,
        num_train_epochs=train_config.max_epochs,
        weight_decay=train_config.weight_decay,
        fp16=train_config.fp16,
        gradient_checkpointing=train_config.gradient_checkpointing,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        logging_steps=50,
        save_total_limit=2,
        report_to="none",
        optim="adamw_torch",
        dataloader_num_workers=4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.1
    )

    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        compute_metrics=lambda p: {"accuracy": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))},
        callbacks=[EarlyStoppingCallback(early_stopping_patience=train_config.early_stop_patience)]
    )

    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš‚ å¯åŠ¨è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹ï¼ˆå…³é”®ä¿®æ”¹ï¼‰
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    final_model_path = os.path.join(train_config.output_dir, "best_model")
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹é…ç½®
    config.save_pretrained(final_model_path)
    # ä¿å­˜é€‚é…å™¨æƒé‡
    model.save_pretrained(final_model_path)
    # ä¿å­˜Tokenizer
    tokenizer.save_pretrained(final_model_path)
    
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{final_model_path}")

if __name__ == "__main__":
    # æ˜¾å­˜ä¼˜åŒ–é…ç½®
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
    
    main()