{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18c8af7",
   "metadata": {},
   "source": [
    "### 加载模型测试，并打印模型，确认最后一层分类层名字，lora微调参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011ad62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForSequenceClassification(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=896, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 设置代理（替换为你的实际代理地址）\n",
    "proxies = {\n",
    "    \"http\": \"http://127.0.0.1:7890\",   # 例如：http://127.0.0.1:7890\n",
    "    \"https\": \"http://127.0.0.1:7890\",  # 例如：http://127.0.0.1:7890\n",
    "}\n",
    "model_name = \"qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "# 下载并加载模型和分词器（显式传递代理）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    proxies=proxies\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    proxies=proxies\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd674c5",
   "metadata": {},
   "source": [
    "### 完整的lora微调qwen代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0075ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🖥️ 硬件配置检查:\n",
      "PyTorch 版本: 2.6.0+cu118\n",
      "CUDA 可用: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "显存: 4.0GB\n",
      "\n",
      "📂 加载数据集...\n",
      "训练集: 60 条, 验证集: 20 条\n",
      "\n",
      "📊 文本长度分析:\n",
      "最小: 5, 最大: 12, 平均: 8.0\n",
      "75% 分位数: 9.0, 95% 分位数: 11.0\n",
      "推荐 max_length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (num_proc=4): 100%|██████████| 60/60 [00:07<00:00,  8.56 examples/s]\n",
      "Tokenizing (num_proc=4): 100%|██████████| 20/20 [00:06<00:00,  3.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 加载预训练模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎛️ 应用 LoRA 配置...\n",
      "trainable params: 1,083,136 || all params: 495,117,696 || trainable%: 0.2188\n",
      "\n",
      "🔍 可训练参数检查:\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "可训练参数: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "可训练参数: base_model.model.score.modules_to_save.default.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚂 启动微调...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.451737</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.451737</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.451737</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 保存最佳模型...\n",
      "LoRA 权重保存至: ./lora_weights/qwen\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import model_config_qwen  # 自定义配置文件，需根据实际情况修改\n",
    "import os\n",
    "\n",
    "# 环境配置\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"TRUST_REMOTE_CODE\"] = \"true\"\n",
    "model_name = \"qwen/Qwen2-0.5B-Instruct\"\n",
    "# 代理设置（根据需要配置）\n",
    "proxies = {\n",
    "    \"http\": \"http://127.0.0.1:7890\",\n",
    "    \"https\": \"http://127.0.0.1:7890\",\n",
    "}\n",
    "\n",
    "# ========== 配置类 ==========\n",
    "class ModelConfig:\n",
    "    model_path = model_config_qwen.model_name_tokenizer_path   # 模型保存路径\n",
    "    num_labels = model_config_qwen.num_labels                 # 分类类别数\n",
    "    model_min_length = model_config_qwen.model_min_length     # 最小输入长度\n",
    "    freeze_base_model = model_config_qwen.freeze_base_model   # 是否冻结基础模型参数（只训练 LoRA 参数）\n",
    "    pooling_type = model_config_qwen.pooling_type             # 池化方式（如 last, first, mean）\n",
    "\n",
    "class TrainingConfig:\n",
    "    output_dir = model_config_qwen.output_dir                 # 输出目录\n",
    "    max_epochs = model_config_qwen.max_epochs                 # 最大训练轮次\n",
    "    batch_size = model_config_qwen.train_batch_size           # 训练批次大小\n",
    "    eval_batch_size = model_config_qwen.test_batch_size         # 验证批次大小\n",
    "    learning_rate = model_config_qwen.learning_rate           # 学习率\n",
    "    weight_decay = model_config_qwen.weight_decay             # 权重衰减\n",
    "    gradient_accumulation = model_config_qwen.gradient_accumulation_steps  # 梯度累积步数\n",
    "    fp16 = model_config_qwen.use_fp16                         # 是否启用混合精度\n",
    "    gradient_checkpointing = model_config_qwen.gradient_checkpointing  # 是否启用梯度检查点\n",
    "    early_stop_patience = model_config_qwen.early_stop_patience           # 早停耐心值\n",
    "\n",
    "class LoRAConfig:\n",
    "    r = model_config_qwen.lora_r                  # LoRA 秩\n",
    "    lora_alpha = model_config_qwen.lora_alpha     # LoRA alpha 值\n",
    "    lora_dropout = model_config_qwen.lora_dropout # LoRA dropout 率\n",
    "    target_modules = model_config_qwen.target_modules  # LoRA 所针对的目标模块列表\n",
    "\n",
    "# ========== 工具函数 ==========\n",
    "def analyze_text_length(texts, tokenizer):\n",
    "    \"\"\"分析文本长度分布，并动态确定 max_length\"\"\"\n",
    "    lengths = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        lengths.append(len(tokens))\n",
    "    print(f\"\\n📊 文本长度分析:\")\n",
    "    print(f\"最小: {min(lengths)}, 最大: {max(lengths)}, 平均: {np.mean(lengths):.1f}\")\n",
    "    print(f\"75% 分位数: {np.percentile(lengths, 75)}, 95% 分位数: {np.percentile(lengths, 95)}\")\n",
    "    max_length = min(int(np.percentile(lengths, 95)), tokenizer.model_max_length)\n",
    "    max_length = max(max_length, ModelConfig.model_min_length)\n",
    "    print(f\"推荐 max_length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "def prepare_dataset(tokenizer, texts, labels, max_length):\n",
    "    \"\"\"数据集预处理，将文本 token 化，同时保留标签\"\"\"\n",
    "    dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "    \n",
    "    def tokenize_fn(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"][0],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][0],\n",
    "            \"label\": examples[\"label\"]\n",
    "        }\n",
    "    \n",
    "    return dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=False,\n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=4,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "\n",
    "# ========== 主流程 ==========\n",
    "def main():\n",
    "    # 硬件检查\n",
    "    print(f\"\\n🖥️ 硬件配置检查:\")\n",
    "    print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "    print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "\n",
    "    # 初始化配置\n",
    "    model_config = ModelConfig()\n",
    "    train_config = TrainingConfig()\n",
    "    lora_config = LoRAConfig()\n",
    "\n",
    "    # 加载数据\n",
    "    print(\"\\n📂 加载数据集...\")\n",
    "    train_df = pd.read_excel(model_config_qwen.train_data_path)\n",
    "    valid_df = pd.read_excel(model_config_qwen.dev_data_path)\n",
    "    print(f\"训练集: {len(train_df)} 条, 验证集: {len(valid_df)} 条\")\n",
    "\n",
    "    # 初始化 Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        proxies=proxies,\n",
    "        pad_token=\"<|endoftext|>\"\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 动态确定 max_length\n",
    "    max_length = analyze_text_length(train_df.iloc[:, 0].tolist(), tokenizer)\n",
    "\n",
    "    # 准备数据集\n",
    "    train_dataset = prepare_dataset(\n",
    "        tokenizer,\n",
    "        train_df.iloc[:, 0].tolist(),\n",
    "        train_df.iloc[:, 1].tolist(),\n",
    "        max_length\n",
    "    )\n",
    "    valid_dataset = prepare_dataset(\n",
    "        tokenizer,\n",
    "        valid_df.iloc[:, 0].tolist(),\n",
    "        valid_df.iloc[:, 1].tolist(),\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "    # 加载预训练模型\n",
    "    print(\"\\n🚀 加载预训练模型...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=model_config.num_labels,\n",
    "        id2label={\"0\": \"非垃圾短信\", \"1\": \"垃圾短信\"},\n",
    "        label2id={\"非垃圾短信\": 0, \"垃圾短信\": 1},\n",
    "        # torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        proxies=proxies,\n",
    "        use_cache=False  # 禁用缓存以兼容梯度检查点\n",
    "    )\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # 应用 LoRA 微调配置\n",
    "    print(\"\\n🎛️ 应用 LoRA 配置...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        target_modules=lora_config.target_modules,\n",
    "        inference_mode=False,\n",
    "        r=lora_config.r,\n",
    "        lora_alpha=lora_config.lora_alpha,\n",
    "        lora_dropout=lora_config.lora_dropout,\n",
    "        modules_to_save=[\"score\"]  # 保留分类头参数可训练\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # 检查可训练参数\n",
    "    print(\"\\n🔍 可训练参数检查:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"可训练参数: {name}\")\n",
    "\n",
    "    # 配置训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=train_config.output_dir,\n",
    "        eval_strategy=\"epoch\",  # 修正参数名称为evaluation_strategy，报错就改为eval_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=train_config.learning_rate,\n",
    "        per_device_train_batch_size=train_config.batch_size,\n",
    "        per_device_eval_batch_size=train_config.eval_batch_size,\n",
    "        gradient_accumulation_steps=train_config.gradient_accumulation,\n",
    "        num_train_epochs=train_config.max_epochs,\n",
    "        weight_decay=train_config.weight_decay,\n",
    "        fp16=train_config.fp16,\n",
    "        gradient_checkpointing=train_config.gradient_checkpointing,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        optim=\"adamw_torch\",\n",
    "        dataloader_num_workers=4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1\n",
    "    )\n",
    "\n",
    "    # 评估函数（计算准确率）\n",
    "    def compute_metrics(p):\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "    # 构建 Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=train_config.early_stop_patience)]\n",
    "    )\n",
    "\n",
    "    # 开始微调训练\n",
    "    print(\"\\n🚂 启动微调...\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # 保存最佳模型和 LoRA 权重\n",
    "    print(\"\\n💾 保存最佳模型...\")\n",
    "    best_model_dir = os.path.join(train_config.output_dir, \"best_model\")\n",
    "    model.save_pretrained(best_model_dir)\n",
    "    tokenizer.save_pretrained(best_model_dir)\n",
    "    lora_model_dir = model_config_qwen.model_path_lora\n",
    "    model.save_pretrained(lora_model_dir)\n",
    "    print(f\"LoRA 权重保存至: {lora_model_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # 启用 TF32 矩阵运算\n",
    "    torch.backends.cudnn.benchmark = True         # 启用 cuDNN 基准优化\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"  # 减少显存碎片\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75199b",
   "metadata": {},
   "source": [
    "### 推理代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d70805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果: 垃圾短信\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import model_config_qwen  # 自定义配置文件，需根据实际情况修改\n",
    "\n",
    "# 加载模型配置\n",
    "peft_model_id = model_config_qwen.model_path_lora  # LoRA 权重保存路径\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# 初始化 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "# 加载基础模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    num_labels=model_config_qwen.num_labels,\n",
    "    id2label={\"0\": \"非垃圾短信\", \"1\": \"垃圾短信\"},\n",
    "    label2id={\"非垃圾短信\": 0, \"垃圾短信\": 1},\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 加载 LoRA 权重\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "\n",
    "# 切换到推理模式\n",
    "model.eval()\n",
    "\n",
    "# 推理函数\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "    return model.config.id2label[str(predicted_class_id)]  # 确保键是字符串类型\n",
    "\n",
    "# 测试推理\n",
    "text = \"这是一个测试文本，判断是否为垃圾短信。\"\n",
    "result = predict(text)\n",
    "print(f\"预测结果: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ca7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_finetune_cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
