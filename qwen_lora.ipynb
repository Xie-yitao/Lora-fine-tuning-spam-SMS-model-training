{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18c8af7",
   "metadata": {},
   "source": [
    "### åŠ è½½æ¨¡å‹æµ‹è¯•ï¼Œå¹¶æ‰“å°æ¨¡å‹ï¼Œç¡®è®¤æœ€åä¸€å±‚åˆ†ç±»å±‚åå­—ï¼Œloraå¾®è°ƒå‚æ•°è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011ad62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForSequenceClassification(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=896, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# è®¾ç½®ä»£ç†ï¼ˆæ›¿æ¢ä¸ºä½ çš„å®é™…ä»£ç†åœ°å€ï¼‰\n",
    "proxies = {\n",
    "    \"http\": \"http://127.0.0.1:7890\",   # ä¾‹å¦‚ï¼šhttp://127.0.0.1:7890\n",
    "    \"https\": \"http://127.0.0.1:7890\",  # ä¾‹å¦‚ï¼šhttp://127.0.0.1:7890\n",
    "}\n",
    "model_name = \"qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ˜¾å¼ä¼ é€’ä»£ç†ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    proxies=proxies\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    proxies=proxies\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd674c5",
   "metadata": {},
   "source": [
    "### å®Œæ•´çš„loraå¾®è°ƒqwenä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0075ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ–¥ï¸ ç¡¬ä»¶é…ç½®æ£€æŸ¥:\n",
      "PyTorch ç‰ˆæœ¬: 2.6.0+cu118\n",
      "CUDA å¯ç”¨: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "æ˜¾å­˜: 4.0GB\n",
      "\n",
      "ğŸ“‚ åŠ è½½æ•°æ®é›†...\n",
      "è®­ç»ƒé›†: 60 æ¡, éªŒè¯é›†: 20 æ¡\n",
      "\n",
      "ğŸ“Š æ–‡æœ¬é•¿åº¦åˆ†æ:\n",
      "æœ€å°: 5, æœ€å¤§: 12, å¹³å‡: 8.0\n",
      "75% åˆ†ä½æ•°: 9.0, 95% åˆ†ä½æ•°: 11.0\n",
      "æ¨è max_length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:07<00:00,  8.56 examples/s]\n",
      "Tokenizing (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:06<00:00,  3.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ›ï¸ åº”ç”¨ LoRA é…ç½®...\n",
      "trainable params: 1,083,136 || all params: 495,117,696 || trainable%: 0.2188\n",
      "\n",
      "ğŸ” å¯è®­ç»ƒå‚æ•°æ£€æŸ¥:\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "å¯è®­ç»ƒå‚æ•°: base_model.model.score.modules_to_save.default.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš‚ å¯åŠ¨å¾®è°ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.451737</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.451737</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.451737</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹...\n",
      "LoRA æƒé‡ä¿å­˜è‡³: ./lora_weights/qwen\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import model_config_qwen  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹\n",
    "import os\n",
    "\n",
    "# ç¯å¢ƒé…ç½®\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"TRUST_REMOTE_CODE\"] = \"true\"\n",
    "model_name = \"qwen/Qwen2-0.5B-Instruct\"\n",
    "# ä»£ç†è®¾ç½®ï¼ˆæ ¹æ®éœ€è¦é…ç½®ï¼‰\n",
    "proxies = {\n",
    "    \"http\": \"http://127.0.0.1:7890\",\n",
    "    \"https\": \"http://127.0.0.1:7890\",\n",
    "}\n",
    "\n",
    "# ========== é…ç½®ç±» ==========\n",
    "class ModelConfig:\n",
    "    model_path = model_config_qwen.model_name_tokenizer_path   # æ¨¡å‹ä¿å­˜è·¯å¾„\n",
    "    num_labels = model_config_qwen.num_labels                 # åˆ†ç±»ç±»åˆ«æ•°\n",
    "    model_min_length = model_config_qwen.model_min_length     # æœ€å°è¾“å…¥é•¿åº¦\n",
    "    freeze_base_model = model_config_qwen.freeze_base_model   # æ˜¯å¦å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°ï¼ˆåªè®­ç»ƒ LoRA å‚æ•°ï¼‰\n",
    "    pooling_type = model_config_qwen.pooling_type             # æ± åŒ–æ–¹å¼ï¼ˆå¦‚ last, first, meanï¼‰\n",
    "\n",
    "class TrainingConfig:\n",
    "    output_dir = model_config_qwen.output_dir                 # è¾“å‡ºç›®å½•\n",
    "    max_epochs = model_config_qwen.max_epochs                 # æœ€å¤§è®­ç»ƒè½®æ¬¡\n",
    "    batch_size = model_config_qwen.train_batch_size           # è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "    eval_batch_size = model_config_qwen.test_batch_size         # éªŒè¯æ‰¹æ¬¡å¤§å°\n",
    "    learning_rate = model_config_qwen.learning_rate           # å­¦ä¹ ç‡\n",
    "    weight_decay = model_config_qwen.weight_decay             # æƒé‡è¡°å‡\n",
    "    gradient_accumulation = model_config_qwen.gradient_accumulation_steps  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    fp16 = model_config_qwen.use_fp16                         # æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦\n",
    "    gradient_checkpointing = model_config_qwen.gradient_checkpointing  # æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "    early_stop_patience = model_config_qwen.early_stop_patience           # æ—©åœè€å¿ƒå€¼\n",
    "\n",
    "class LoRAConfig:\n",
    "    r = model_config_qwen.lora_r                  # LoRA ç§©\n",
    "    lora_alpha = model_config_qwen.lora_alpha     # LoRA alpha å€¼\n",
    "    lora_dropout = model_config_qwen.lora_dropout # LoRA dropout ç‡\n",
    "    target_modules = model_config_qwen.target_modules  # LoRA æ‰€é’ˆå¯¹çš„ç›®æ ‡æ¨¡å—åˆ—è¡¨\n",
    "\n",
    "# ========== å·¥å…·å‡½æ•° ==========\n",
    "def analyze_text_length(texts, tokenizer):\n",
    "    \"\"\"åˆ†ææ–‡æœ¬é•¿åº¦åˆ†å¸ƒï¼Œå¹¶åŠ¨æ€ç¡®å®š max_length\"\"\"\n",
    "    lengths = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        lengths.append(len(tokens))\n",
    "    print(f\"\\nğŸ“Š æ–‡æœ¬é•¿åº¦åˆ†æ:\")\n",
    "    print(f\"æœ€å°: {min(lengths)}, æœ€å¤§: {max(lengths)}, å¹³å‡: {np.mean(lengths):.1f}\")\n",
    "    print(f\"75% åˆ†ä½æ•°: {np.percentile(lengths, 75)}, 95% åˆ†ä½æ•°: {np.percentile(lengths, 95)}\")\n",
    "    max_length = min(int(np.percentile(lengths, 95)), tokenizer.model_max_length)\n",
    "    max_length = max(max_length, ModelConfig.model_min_length)\n",
    "    print(f\"æ¨è max_length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "def prepare_dataset(tokenizer, texts, labels, max_length):\n",
    "    \"\"\"æ•°æ®é›†é¢„å¤„ç†ï¼Œå°†æ–‡æœ¬ token åŒ–ï¼ŒåŒæ—¶ä¿ç•™æ ‡ç­¾\"\"\"\n",
    "    dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "    \n",
    "    def tokenize_fn(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"][0],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][0],\n",
    "            \"label\": examples[\"label\"]\n",
    "        }\n",
    "    \n",
    "    return dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=False,\n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=4,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "\n",
    "# ========== ä¸»æµç¨‹ ==========\n",
    "def main():\n",
    "    # ç¡¬ä»¶æ£€æŸ¥\n",
    "    print(f\"\\nğŸ–¥ï¸ ç¡¬ä»¶é…ç½®æ£€æŸ¥:\")\n",
    "    print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "\n",
    "    # åˆå§‹åŒ–é…ç½®\n",
    "    model_config = ModelConfig()\n",
    "    train_config = TrainingConfig()\n",
    "    lora_config = LoRAConfig()\n",
    "\n",
    "    # åŠ è½½æ•°æ®\n",
    "    print(\"\\nğŸ“‚ åŠ è½½æ•°æ®é›†...\")\n",
    "    train_df = pd.read_excel(model_config_qwen.train_data_path)\n",
    "    valid_df = pd.read_excel(model_config_qwen.dev_data_path)\n",
    "    print(f\"è®­ç»ƒé›†: {len(train_df)} æ¡, éªŒè¯é›†: {len(valid_df)} æ¡\")\n",
    "\n",
    "    # åˆå§‹åŒ– Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        proxies=proxies,\n",
    "        pad_token=\"<|endoftext|>\"\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # åŠ¨æ€ç¡®å®š max_length\n",
    "    max_length = analyze_text_length(train_df.iloc[:, 0].tolist(), tokenizer)\n",
    "\n",
    "    # å‡†å¤‡æ•°æ®é›†\n",
    "    train_dataset = prepare_dataset(\n",
    "        tokenizer,\n",
    "        train_df.iloc[:, 0].tolist(),\n",
    "        train_df.iloc[:, 1].tolist(),\n",
    "        max_length\n",
    "    )\n",
    "    valid_dataset = prepare_dataset(\n",
    "        tokenizer,\n",
    "        valid_df.iloc[:, 0].tolist(),\n",
    "        valid_df.iloc[:, 1].tolist(),\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "    print(\"\\nğŸš€ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=model_config.num_labels,\n",
    "        id2label={\"0\": \"éåƒåœ¾çŸ­ä¿¡\", \"1\": \"åƒåœ¾çŸ­ä¿¡\"},\n",
    "        label2id={\"éåƒåœ¾çŸ­ä¿¡\": 0, \"åƒåœ¾çŸ­ä¿¡\": 1},\n",
    "        # torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        proxies=proxies,\n",
    "        use_cache=False  # ç¦ç”¨ç¼“å­˜ä»¥å…¼å®¹æ¢¯åº¦æ£€æŸ¥ç‚¹\n",
    "    )\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # åº”ç”¨ LoRA å¾®è°ƒé…ç½®\n",
    "    print(\"\\nğŸ›ï¸ åº”ç”¨ LoRA é…ç½®...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        target_modules=lora_config.target_modules,\n",
    "        inference_mode=False,\n",
    "        r=lora_config.r,\n",
    "        lora_alpha=lora_config.lora_alpha,\n",
    "        lora_dropout=lora_config.lora_dropout,\n",
    "        modules_to_save=[\"score\"]  # ä¿ç•™åˆ†ç±»å¤´å‚æ•°å¯è®­ç»ƒ\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°\n",
    "    print(\"\\nğŸ” å¯è®­ç»ƒå‚æ•°æ£€æŸ¥:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"å¯è®­ç»ƒå‚æ•°: {name}\")\n",
    "\n",
    "    # é…ç½®è®­ç»ƒå‚æ•°\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=train_config.output_dir,\n",
    "        eval_strategy=\"epoch\",  # ä¿®æ­£å‚æ•°åç§°ä¸ºevaluation_strategyï¼ŒæŠ¥é”™å°±æ”¹ä¸ºeval_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=train_config.learning_rate,\n",
    "        per_device_train_batch_size=train_config.batch_size,\n",
    "        per_device_eval_batch_size=train_config.eval_batch_size,\n",
    "        gradient_accumulation_steps=train_config.gradient_accumulation,\n",
    "        num_train_epochs=train_config.max_epochs,\n",
    "        weight_decay=train_config.weight_decay,\n",
    "        fp16=train_config.fp16,\n",
    "        gradient_checkpointing=train_config.gradient_checkpointing,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        optim=\"adamw_torch\",\n",
    "        dataloader_num_workers=4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1\n",
    "    )\n",
    "\n",
    "    # è¯„ä¼°å‡½æ•°ï¼ˆè®¡ç®—å‡†ç¡®ç‡ï¼‰\n",
    "    def compute_metrics(p):\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "    # æ„å»º Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=train_config.early_stop_patience)]\n",
    "    )\n",
    "\n",
    "    # å¼€å§‹å¾®è°ƒè®­ç»ƒ\n",
    "    print(\"\\nğŸš‚ å¯åŠ¨å¾®è°ƒ...\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œ LoRA æƒé‡\n",
    "    print(\"\\nğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹...\")\n",
    "    best_model_dir = os.path.join(train_config.output_dir, \"best_model\")\n",
    "    model.save_pretrained(best_model_dir)\n",
    "    tokenizer.save_pretrained(best_model_dir)\n",
    "    lora_model_dir = model_config_qwen.model_path_lora\n",
    "    model.save_pretrained(lora_model_dir)\n",
    "    print(f\"LoRA æƒé‡ä¿å­˜è‡³: {lora_model_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨ TF32 çŸ©é˜µè¿ç®—\n",
    "    torch.backends.cudnn.benchmark = True         # å¯ç”¨ cuDNN åŸºå‡†ä¼˜åŒ–\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"  # å‡å°‘æ˜¾å­˜ç¢ç‰‡\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75199b",
   "metadata": {},
   "source": [
    "### æ¨ç†ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d70805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 64 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at qwen/Qwen2-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Anaconda3\\envs\\qwen_finetune_cls\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢„æµ‹ç»“æœ: åƒåœ¾çŸ­ä¿¡\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import model_config_qwen  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹\n",
    "\n",
    "# åŠ è½½æ¨¡å‹é…ç½®\n",
    "peft_model_id = model_config_qwen.model_path_lora  # LoRA æƒé‡ä¿å­˜è·¯å¾„\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# åˆå§‹åŒ– Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "# åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    num_labels=model_config_qwen.num_labels,\n",
    "    id2label={\"0\": \"éåƒåœ¾çŸ­ä¿¡\", \"1\": \"åƒåœ¾çŸ­ä¿¡\"},\n",
    "    label2id={\"éåƒåœ¾çŸ­ä¿¡\": 0, \"åƒåœ¾çŸ­ä¿¡\": 1},\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# åŠ è½½ LoRA æƒé‡\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "\n",
    "# åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼\n",
    "model.eval()\n",
    "\n",
    "# æ¨ç†å‡½æ•°\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "    return model.config.id2label[str(predicted_class_id)]  # ç¡®ä¿é”®æ˜¯å­—ç¬¦ä¸²ç±»å‹\n",
    "\n",
    "# æµ‹è¯•æ¨ç†\n",
    "text = \"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºåƒåœ¾çŸ­ä¿¡ã€‚\"\n",
    "result = predict(text)\n",
    "print(f\"é¢„æµ‹ç»“æœ: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ca7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_finetune_cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
